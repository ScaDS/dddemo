<!--
  tab-name: Evaluation Approaches
  order: 3
-->
<div class="slide-content">
  <h3>Evaluation Approaches</h3>

  <div class="paper-cards" style="margin-bottom: 1.5rem;">
    <div class="paper-card">
      <div class="paper-year">2025</div>
      <div class="paper-body">
        <div class="paper-title">A benchmark and survey of fully unsupervised concept drift detectors on real-world data streams</div>
        <div class="paper-authors">Lukats, D., Zielinski, O., Hahn, A., &amp; Stahl, F.</div>
        <div class="paper-venue">International Journal of Data Science and Analytics, 19(1), pp. 1â€“31, Springer</div>
      </div>
    </div>
  </div>

  <div style="display: flex; gap: 2rem; align-items: center;">
    <p class="slide-intro" style="flex: 1; margin-bottom: 0;">Lukats et al., 2025 discuss different evaluation approaches for concept drift detection.
      They also highlight the complexity of the evaluation process when using real-world and synthetic data.
      <br>
      The plot on the right reveals that there is a sweet spot in the number of detected drifts and the accuracy of the classifier.
      <br><br>
      Interested in running or adapting the benchmark? Check out the project's <a href="https://github.com/DFKI-NI/unsupervised-concept-drift-detection" target="_blank">GitHub repository</a>.
    </p>

    <div style="flex: 1;">
      <img src="content/research/evaluation-approaches/acc_dd.png" alt="Evaluation approaches for drift detection" style="width: 100%; border-radius: 12px; box-shadow: var(--shadow-lg);">
    </div>
  </div>
</div>
